{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 782819 characters\n",
      "90 unique characters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_to_file = \"corpus.txt\"\n",
    "\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text100'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_29'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4983\n",
      "Epoch 1 Batch 100 Loss 2.1247\n",
      "Epoch 1 Loss 2.0078\n",
      "Time taken for 1 epoch 7.400284051895142 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.0573\n",
      "Epoch 2 Batch 100 Loss 1.4922\n",
      "Epoch 2 Loss 1.4054\n",
      "Time taken for 1 epoch 6.048375606536865 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3572\n",
      "Epoch 3 Batch 100 Loss 1.1047\n",
      "Epoch 3 Loss 1.1403\n",
      "Time taken for 1 epoch 6.46974778175354 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.0895\n",
      "Epoch 4 Batch 100 Loss 1.0668\n",
      "Epoch 4 Loss 1.0479\n",
      "Time taken for 1 epoch 6.097682237625122 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9669\n",
      "Epoch 5 Batch 100 Loss 0.9701\n",
      "Epoch 5 Loss 0.9239\n",
      "Time taken for 1 epoch 7.480591773986816 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9585\n",
      "Epoch 6 Batch 100 Loss 0.9351\n",
      "Epoch 6 Loss 0.9184\n",
      "Time taken for 1 epoch 8.796870470046997 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9061\n",
      "Epoch 7 Batch 100 Loss 0.9636\n",
      "Epoch 7 Loss 0.8620\n",
      "Time taken for 1 epoch 6.546564817428589 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8357\n",
      "Epoch 8 Batch 100 Loss 0.8785\n",
      "Epoch 8 Loss 0.8457\n",
      "Time taken for 1 epoch 9.292232275009155 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7633\n",
      "Epoch 9 Batch 100 Loss 0.7919\n",
      "Epoch 9 Loss 0.7784\n",
      "Time taken for 1 epoch 6.2043750286102295 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.7249\n",
      "Epoch 10 Batch 100 Loss 0.7826\n",
      "Epoch 10 Loss 0.7828\n",
      "Time taken for 1 epoch 8.264200687408447 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.6611\n",
      "Epoch 11 Batch 100 Loss 0.7083\n",
      "Epoch 11 Loss 0.7552\n",
      "Time taken for 1 epoch 6.49015736579895 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.7049\n",
      "Epoch 12 Batch 100 Loss 0.7365\n",
      "Epoch 12 Loss 0.7541\n",
      "Time taken for 1 epoch 7.435153961181641 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.6592\n",
      "Epoch 13 Batch 100 Loss 0.7399\n",
      "Epoch 13 Loss 0.6885\n",
      "Time taken for 1 epoch 6.360558748245239 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6456\n",
      "Epoch 14 Batch 100 Loss 0.7129\n",
      "Epoch 14 Loss 0.6505\n",
      "Time taken for 1 epoch 8.290943622589111 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.5985\n",
      "Epoch 15 Batch 100 Loss 0.7116\n",
      "Epoch 15 Loss 0.6552\n",
      "Time taken for 1 epoch 7.921302795410156 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5971\n",
      "Epoch 16 Batch 100 Loss 0.6512\n",
      "Epoch 16 Loss 0.6588\n",
      "Time taken for 1 epoch 6.093705177307129 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5618\n",
      "Epoch 17 Batch 100 Loss 0.5892\n",
      "Epoch 17 Loss 0.6333\n",
      "Time taken for 1 epoch 7.756326913833618 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5667\n",
      "Epoch 18 Batch 100 Loss 0.6188\n",
      "Epoch 18 Loss 0.6085\n",
      "Time taken for 1 epoch 6.6406190395355225 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5537\n",
      "Epoch 19 Batch 100 Loss 0.5681\n",
      "Epoch 19 Loss 0.5884\n",
      "Time taken for 1 epoch 7.746066331863403 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5271\n",
      "Epoch 20 Batch 100 Loss 0.5788\n",
      "Epoch 20 Loss 0.5567\n",
      "Time taken for 1 epoch 7.249527454376221 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4969\n",
      "Epoch 21 Batch 100 Loss 0.5790\n",
      "Epoch 21 Loss 0.5586\n",
      "Time taken for 1 epoch 8.105046033859253 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.4913\n",
      "Epoch 22 Batch 100 Loss 0.5499\n",
      "Epoch 22 Loss 0.5412\n",
      "Time taken for 1 epoch 6.083613395690918 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.4679\n",
      "Epoch 23 Batch 100 Loss 0.5192\n",
      "Epoch 23 Loss 0.5531\n",
      "Time taken for 1 epoch 6.611499309539795 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4665\n",
      "Epoch 24 Batch 100 Loss 0.5282\n",
      "Epoch 24 Loss 0.5342\n",
      "Time taken for 1 epoch 6.240666151046753 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.4570\n",
      "Epoch 25 Batch 100 Loss 0.5158\n",
      "Epoch 25 Loss 0.4841\n",
      "Time taken for 1 epoch 8.662562131881714 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.4488\n",
      "Epoch 26 Batch 100 Loss 0.4887\n",
      "Epoch 26 Loss 0.5073\n",
      "Time taken for 1 epoch 6.185457468032837 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4313\n",
      "Epoch 27 Batch 100 Loss 0.4746\n",
      "Epoch 27 Loss 0.5016\n",
      "Time taken for 1 epoch 6.128339529037476 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.3925\n",
      "Epoch 28 Batch 100 Loss 0.4882\n",
      "Epoch 28 Loss 0.5009\n",
      "Time taken for 1 epoch 6.159857273101807 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.3903\n",
      "Epoch 29 Batch 100 Loss 0.4814\n",
      "Epoch 29 Loss 0.4667\n",
      "Time taken for 1 epoch 6.38183069229126 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.3980\n",
      "Epoch 30 Batch 100 Loss 0.4568\n",
      "Epoch 30 Loss 0.4540\n",
      "Time taken for 1 epoch 9.642849922180176 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions = model(inp)\n",
    "              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "          if batch_n % 100 == 0:\n",
    "              template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "              print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_29'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            23040     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 90)             92250     \n",
      "=================================================================\n",
      "Total params: 4,053,594\n",
      "Trainable params: 4,053,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  #num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .9\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  test = []\n",
    "  while test != '\\n':#for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      test=idx2char[predicted_id]\n",
    "      text_generated.append(test)\n",
    "    \n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change inputw to what you would like to draw\n",
    "inputw='Draw a circle'\n",
    "\n",
    "output_instruction = ''\n",
    "output_instruction+=(generate_text(model, \"1. \"  + inputw))\n",
    "for i in range(2,11):\n",
    "   output_instruction=(generate_text(model, output_instruction+str(i)+'.'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Draw a circle overlapping the tip of the saw, completing thes of slime. Draw in the rectangle unco ne’ses of the foot, then use a curved line to outline a rad im of the toe cab. That in our example, we’r drawing of the popsicle to a curved line towards the centre of the house. Finally, use two curved lines to enclose the shape of the elbow and above the open mouth.\n",
      "2. ​​​​​​​​​Add features to the suinect the lines overlap. Draw a series of connected, curved lines to form the toes.\n",
      "3. Extend two curved lines from each side of the house, and enclose the figure using a curved line. Then, extend a pair of short, parallel lines across the bottom section, the face is called the neck. Note how the line curves near the front of the hat.\n",
      "4. ​​​​​​​​​Draw a straight line downward from the end of the brim. Then, use straight lines to enclose the figure by using a series of connected, curved lines to sketch the boot’s interesting.\n",
      "5. Draw a scalloped line to form the cuff using a curved line. Then, extend a short, curved line from the opposite side of the peg or side, then use a shade the guide line formed by the circle at the top, completing the tops of hair. Draw a long, curved line beneath the nose. Connect these using a curved line. Then, draw a curved line across the top of the coffin, using six side, sharpen by the first follow the curvature of the UFO\n",
      "6. Draw a smaller oval within the lines drawn in the previous step and the lower section. Draw a teardrop shape for the legs and feet. Finally, use two curved lines to form tof the forkstone.\n",
      "7. Use a long, curved line to outline a rounded rectangular shape beneath the ovals’s technique – just a mattered them in drawn, then use a wavy line parallel to the first. Again, draw a curved line down the length of the toes.\n",
      "8. Draw the fork in the branches using curved lines. Detail the front of the hat, the large lobe to one curved lines to connect the lines of the eyebrow, and more rolofian Imape.\n",
      "9. Give the dress sits upon. From this point, draw a large, rounded rectangular shape beneath the nose. Then, use long, curved lines to add detail by drawing curved lines of varying length of the popcorn bucket.\n",
      "10. Color your igloo. This forms the lottom of the arrow head.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
