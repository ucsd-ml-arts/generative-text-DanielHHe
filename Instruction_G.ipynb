{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 782819 characters\n",
      "90 unique characters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "path_to_file = \"corpus.txt\"\n",
    "\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text100'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_29'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4993\n",
      "Epoch 1 Batch 100 Loss 2.1265\n",
      "Epoch 1 Loss 1.9905\n",
      "Time taken for 1 epoch 8.148773193359375 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.0355\n",
      "Epoch 2 Batch 100 Loss 1.4726\n",
      "Epoch 2 Loss 1.3557\n",
      "Time taken for 1 epoch 6.432200193405151 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3230\n",
      "Epoch 3 Batch 100 Loss 1.1995\n",
      "Epoch 3 Loss 1.1470\n",
      "Time taken for 1 epoch 6.415666818618774 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1649\n",
      "Epoch 4 Batch 100 Loss 1.0452\n",
      "Epoch 4 Loss 1.0083\n",
      "Time taken for 1 epoch 6.448187351226807 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9670\n",
      "Epoch 5 Batch 100 Loss 0.9523\n",
      "Epoch 5 Loss 0.9329\n",
      "Time taken for 1 epoch 7.229085445404053 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.8667\n",
      "Epoch 6 Batch 100 Loss 0.8636\n",
      "Epoch 6 Loss 0.9522\n",
      "Time taken for 1 epoch 6.648202180862427 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8943\n",
      "Epoch 7 Batch 100 Loss 0.8314\n",
      "Epoch 7 Loss 0.9082\n",
      "Time taken for 1 epoch 6.63148832321167 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7998\n",
      "Epoch 8 Batch 100 Loss 0.8326\n",
      "Epoch 8 Loss 0.9265\n",
      "Time taken for 1 epoch 6.301945209503174 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7906\n",
      "Epoch 9 Batch 100 Loss 0.7694\n",
      "Epoch 9 Loss 0.8005\n",
      "Time taken for 1 epoch 6.59617018699646 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.7704\n",
      "Epoch 10 Batch 100 Loss 0.7507\n",
      "Epoch 10 Loss 0.7684\n",
      "Time taken for 1 epoch 6.685257196426392 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.7553\n",
      "Epoch 11 Batch 100 Loss 0.7613\n",
      "Epoch 11 Loss 0.7278\n",
      "Time taken for 1 epoch 6.600183010101318 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.7114\n",
      "Epoch 12 Batch 100 Loss 0.7463\n",
      "Epoch 12 Loss 0.7450\n",
      "Time taken for 1 epoch 6.506974935531616 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.6307\n",
      "Epoch 13 Batch 100 Loss 0.7391\n",
      "Epoch 13 Loss 0.7063\n",
      "Time taken for 1 epoch 6.690100193023682 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6882\n",
      "Epoch 14 Batch 100 Loss 0.6947\n",
      "Epoch 14 Loss 0.6971\n",
      "Time taken for 1 epoch 6.672601699829102 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.5994\n",
      "Epoch 15 Batch 100 Loss 0.6134\n",
      "Epoch 15 Loss 0.6780\n",
      "Time taken for 1 epoch 7.06689715385437 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5935\n",
      "Epoch 16 Batch 100 Loss 0.6544\n",
      "Epoch 16 Loss 0.6628\n",
      "Time taken for 1 epoch 6.674399375915527 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5979\n",
      "Epoch 17 Batch 100 Loss 0.6392\n",
      "Epoch 17 Loss 0.6488\n",
      "Time taken for 1 epoch 6.4141058921813965 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5651\n",
      "Epoch 18 Batch 100 Loss 0.6301\n",
      "Epoch 18 Loss 0.5791\n",
      "Time taken for 1 epoch 6.4974260330200195 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5589\n",
      "Epoch 19 Batch 100 Loss 0.6066\n",
      "Epoch 19 Loss 0.5845\n",
      "Time taken for 1 epoch 6.460925102233887 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5070\n",
      "Epoch 20 Batch 100 Loss 0.5588\n",
      "Epoch 20 Loss 0.6225\n",
      "Time taken for 1 epoch 6.934601783752441 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4898\n",
      "Epoch 21 Batch 100 Loss 0.5698\n",
      "Epoch 21 Loss 0.5456\n",
      "Time taken for 1 epoch 6.621545314788818 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.4589\n",
      "Epoch 22 Batch 100 Loss 0.5287\n",
      "Epoch 22 Loss 0.5677\n",
      "Time taken for 1 epoch 6.5437257289886475 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.4722\n",
      "Epoch 23 Batch 100 Loss 0.5322\n",
      "Epoch 23 Loss 0.5268\n",
      "Time taken for 1 epoch 6.318851947784424 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4660\n",
      "Epoch 24 Batch 100 Loss 0.5022\n",
      "Epoch 24 Loss 0.5148\n",
      "Time taken for 1 epoch 6.481871843338013 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.4350\n",
      "Epoch 25 Batch 100 Loss 0.4938\n",
      "Epoch 25 Loss 0.5143\n",
      "Time taken for 1 epoch 7.259062767028809 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.4285\n",
      "Epoch 26 Batch 100 Loss 0.4859\n",
      "Epoch 26 Loss 0.4941\n",
      "Time taken for 1 epoch 6.485111236572266 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4022\n",
      "Epoch 27 Batch 100 Loss 0.4551\n",
      "Epoch 27 Loss 0.4816\n",
      "Time taken for 1 epoch 6.456147193908691 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.4243\n",
      "Epoch 28 Batch 100 Loss 0.4771\n",
      "Epoch 28 Loss 0.4751\n",
      "Time taken for 1 epoch 6.481659412384033 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.3828\n",
      "Epoch 29 Batch 100 Loss 0.4555\n",
      "Epoch 29 Loss 0.4536\n",
      "Time taken for 1 epoch 6.590737819671631 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.4034\n",
      "Epoch 30 Batch 100 Loss 0.4304\n",
      "Epoch 30 Loss 0.4602\n",
      "Time taken for 1 epoch 6.924279451370239 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions = model(inp)\n",
    "              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "          if batch_n % 100 == 0:\n",
    "              template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "              print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_29'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            23040     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 90)             92250     \n",
      "=================================================================\n",
      "Total params: 4,053,594\n",
      "Trainable params: 4,053,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  #num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .8\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  test = []\n",
    "  while test != '\\n':#for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      test=idx2char[predicted_id]\n",
    "      text_generated.append(test)\n",
    "    \n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change inputw to what you would like to draw\n",
    "inputw='Draw a line'\n",
    "\n",
    "output_instruction = ''\n",
    "output_instruction+=(generate_text(model, \"1. \"  + inputw))\n",
    "for i in range(2,11):\n",
    "   output_instruction=(generate_text(model, output_instruction+str(i)+'.'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Draw a line and banner to the opposite side, forming a mirror image of the first line, fully enclosing the star and the stem. Atate the spiral shaped lines of the original rectangle.\n",
      "2. ​​​​​​Draw a second egg shape from which it his completes the torso of a fun.\n",
      "3. ​​​​​​​​​​​​​​Draw the ears. Use a curved line to connect the far side of the left line of the ears on purpose to look between the skin striped in the skeleton and bears, oo rode lines of various sizes with rounded corners.\n",
      "4. Outline the leaves, enclose a curved shape at its base – these flames using short lines that meet in jagged points.\n",
      "5. Draw the school bus. Most small squares and circles should be partially ed their the bright colors.\n",
      "6. Draw many curved lines on the opposite side of the bat’s barrel and the sides of the figure to outline the donkey’s muzzle.\n",
      "7. ​​​Draw a straight, horizontal line from the middle of each to meet in a point. Draw curved lines on the opposite side, forming the opposite side of the pencil. Connect the lines from the previous lines end in PDF of the Drawing Guide\n",
      "8. Draw the far front leg, again using long, curved lines.\n",
      "9. Detail the face with curved lines. Indicate a more petals center, a barana. This shape forms yet, Indicate the shape of the cow’s neck and back, of the legs and feet using a short, curved line.\n",
      "10. ​​​​​ ​​​​​Color your hands.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
